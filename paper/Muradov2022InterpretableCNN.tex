\documentclass[12pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}




\title{Выбор интерпретируемых сверточных моделей глубокого обучения}

\author{ Тимур Мурадов\\
	МФТИ\\
	\And
	Олег Бахтеев \\
	МФТИ\\
	\And
	Константин Яковлев \\
	МФТИ\\
	\And
	Вадим Стрижов \\
	МФТИ\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\undertitle}{}
\renewcommand{\headeright}{}
\renewcommand{\shorttitle}{Выбор интерпретируемых сверточных моделей глубокого обучения}

\hypersetup{
pdftitle={Выбор интерпретируемых сверточных моделей глубокого обучения},
pdfauthor={Тимур Мурадов},
pdfkeywords={Model interpretability \and Deep Learning \and OpenBox \and Convolutional neural networks},
}


\begin{document}
\maketitle

\begin{abstract}
	В статье рассматривается задача построения интерпретируемой сверточной нейронной сети. Под интерпретируемостью модели понимается выделение наиболее важных признаков и определение кластеров схожих объектов. Для повышения интерпретируемоси в статье вводится модификация метода OpenBox работающего с  кусочно-линейными нейронными сетями. В нём модель представляется в виде набора интерпретируемых линейных классификаторов. Каждый из них определен на выпуклом многограннике. Это позволяет классифицировать схожие объекты одним и тем же классификатором. Метод обобщается на работу с более широким классом нейронных сетей: сверточными нейронными сетями. Предлагается математически эквивалентная замена слоев свёрточной сети на линейные модели. Что  значительно повышает интепретируемость. Вычислительный эксперимент проводится на выборках изображений рукописных цифр MNIST и изображений CIFAR-10.
\end{abstract}


\keywords{Model interpretability \and Deep Learning \and OpenBox \and Convolutional neural networks}

\section{Introduction}
В данном исследовании стоит задача повышения интерпретируемости модели, где под интерпретируемостью понимается простота выделения важных признаков на выборке данных и классификация близких объектов одним и тем же классификатором.

Проблемой является в целом высокая сложность интерпретации сверточных нейронных сетей, требующая комплексного подхода. На данный момент существует множество различных решений проблемы интерпретации \cite{ribeiro2016why, Lundberg2017aunified, chu2019exact} . В статье \cite{ribeiro2016why} описан метод \textbf{LIME}, предлагащий линейную апроксимацию предсказаний модели в некоторой небольшой окрестности вокруг объектов из тестовой выборки. Такой подход позволяет получить простую для интерпретации модель без использования информации о строении модели изнутри \textquotedblleft model-agnostic\textquotedblright. Но он весьма неустойчив к выбросам и сильно зависим от точности апроксимации. В статье \cite{Lundberg2017aunified} предлагается подход \textbf{SHAP}, заключающийся в рассмотрении вклада каждого признака в работу модели. Таким образом удается выделять даже скрытые, но значимые признаки. Однако применимость данного подхода ограничена ввиду высоких вычислительных затрат: требуется многократное обучение модели, и он весьма зависим от выборки данных. Ещё один подход к интерпретации \textbf{OpenBox}, описываемый в статье \cite{chu2019exact} предлагает построение математически эквивалентных линейных моделей для линейных нейронных сетей. Он показал более высокую эффективность по сравнению с \textbf{LIME} и весьма перспективен для дальнейшей работы.

В данной работе предлагается адаптация метода \textbf{OpenBox} для работы со свёрточными нейронными сетями: математически эквивалентно представить в виде линейных моделей такие слои как свёртка, пулинг и нормализация. И сравнение с альтернативными методами интепретации CNN.

Для анализа качества предложенного метода проводится вычислительный эксперимент на выборках изображений рукописных цифр MNIST и изображений CIFAR-10.



\section{Problem Definition}
\label{sec:headings}

Задана выборка изображений для классификации.

Для CNN $\mathcal{N}$ содержащей $L$ слоев назовем $l$ слой $\mathcal{L}_l$. Так $\mathcal{L}_1$ - входной слой, а $\mathcal{L}_L$ - выходной слой. Слои со $2$ по $L-1$ - это скрытые слои. На вход сети $\mathcal{N}$ подается $x \in \mathcal{X}$, где $X \subseteq \mathbb{R}^d$, из пространства размерности $d$. 

Слои с $2$ по $L-1$ свёрточной сети $\mathcal{N}$ представляют собой свёртки, пулинги и нормализации. Каждый из которых можно представить в виде линейных операций. Между слоями применяются кусочно линейные функции активации.

На выходе сети $\mathcal{N}$ получается вектор $a \in \mathcal{Y}$, где $Y \subseteq \mathbb{R}^{n_L}$,  пространство размерности $n_L$. Выходной слой сети $\mathcal{L}_l$ применяет функцию softmax для получения выходного вектора. 

CNN работает как классификатор $F: \mathcal{X} \rightarrow \mathcal{Y}$, сопоставляющий элементам из множества $\mathcal{X}$ элементы из множества $\mathcal{Y}$. При этом $F$ представляет собой систему тяжело интерпретируемую человеком.

Требуется построить интерпретацию CNN $\mathcal{N}$ в виде линейной модели $\mathcal{M}$, хорошо поддающейся интерпретации. Таким образом для $\mathcal{M}$ выдвигаются два требования:
\begin{itemize}
    \item \textbf{Точность}:
    Модель $\mathcal{M}$ математически эквивалента модели $\mathcal{N}$.
    \item \textbf{Консистентность}:
    Модель $\mathcal{M}$ даёт близкие интерпретации для близких объектов выборки.
\end{itemize}

\section{Our setup}

Строим CNN и при помощи метода LIME получаем интерпретации признаков модели.

\section{Computational experiment}

Эксперимент заключается в получении baseline качества интрепретаций для дальнейшего исследования.

\subsection{Data}

  Fashion-MNIST датасет содержащий 60000 изображений в train и 10000 изображений в test из 10 различных классов. Каждое изображение имеет разрешение 28*28 пикселей.  \citep{fashionmnist}.

\subsection{Configuration of algorithm run}

Считаем точность предсказаний и расстояние между признаками, полученные при помощи алгоритма LIME.

\subsection{Preliminary report}


\begin{figure}[!t]
  \subfloat{\includegraphics[width=0.5\textwidth]{../figures/accuracy.png}}
  \subfloat{\includegraphics[width=0.5\textwidth]{../figures/similarity.png}}\\
 \caption{Accuracy and cosine similarity between decision features}
  \label{fig:1}
\end{figure}

\newpage

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}