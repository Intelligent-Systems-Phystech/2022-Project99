\documentclass[12pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}




\title{Выбор интерпретируемых сверточных моделей глубокого обучения}

\author{ Тимур Мурадов\\
	МФТИ\\
	\And
	Олег Бахтеев \\
	МФТИ\\
	\And
	Константин Яковлев \\
	МФТИ\\
	\And
	Вадим Стрижов \\
	МФТИ\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arrxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	В статье рассматривается задача построения интерпретируемой сверточной нейронной сети. Под интерпретируемостью модели понимается выделение наиболее важных признаков, а также определение кластеров схожих объектов. Для улучшения интерпретируемоси в статье вводится модификация метода OpenBox работающего с  кусочно-линейными нейронными сетями. В нём модель представляется в виде набора интерпретируемых линейных классификаторов, при этом каждый из них определен на выпуклом многограннике, что позволяет классифицировать схожие объекты одним и тем же классификатором. Метод обобщается на работу с более широким классом нейронных сетей: сверточными нейронными сетями. Предлагается математически эквивалентная замена слоев свёрточной сети на линейные модели, что позволяет значительно улучшить интепретируемость. Вычислительный эксперимент проводится на выборках изображений рукописных цифр MNIST и изображений CIFAR-10.
\end{abstract}


\keywords{Model interpretability \and Deep Learning \and OpenBox \and Convolutional neural networks}

\section{Introduction}
В данном исследовании стоит задача улучшения интерпретируемости модели, где под интерпретируемостью понимается простота выделения важных признаков на выборке данных и способность относить схожие объекты выборки к одним и тем же кластерам.

Проблемой является в целом высокая сложность интерпретации сверточных нейронных сетей, требующая комплексного подхода. На данный момент существует множество различных решений проблемы интерпретации. В статье \cite{ribeiro2016why} описан метод \textbf{LIME}, предлагащий линейную апроксимацию предсказаний модели в некоторой небольшой окрестности вокруг объектов из тестовой выборки. Такой подход позволяет получить простую для интерпретации модель, являясь при этом \textquotedblleft model-agnostic\textquotedblright, то есть никак не использующий информацию о строении модели изнутри. Но он весьма неустойчив к выбросам и сильно зависим от адекватности апроксимации. В статье \cite{Lundberg2017aunified} предлагается другой подход \textbf{SHAP}, заключающийся в рассмотрении вклада каждого признака в результат работы модели. Таким образом удается выделять даже скрытые, но значимые признаки. Однако применимость данного подхода ограничена ввиду высоких вычислительных затрат, требуется многократное обучение модели, а также он весьма зависим от выборки данных. Ещё один подход к интерпретации \textbf{OpenBox}, описываемый в статье \cite{chu2019exact} предлагает построение математически эквивалентных линейных моделей для линейных нейронных сетей. Он показал более высокую эффективность по сравнению с \textbf{LIME} и весьма перспективен для дальнейшей работы.

В данной работе предлагается адаптация метода \textbf{OpenBox} для работы со свёрточными нейронными сетями: математически эквивалентно представить в виде линейных моделей такие слои как свёртка, пулинг и нормализация. И доказательство конкурентоспособности по сравнению с другими существующими методами интепретации CNN.

Для анализа качества предложенного метода проводится вычислительный эксперимент на выборках изображений рукописных цифр MNIST и изображений CIFAR-10.



\section{Problem Definition}
\label{sec:headings}

Для CNN $\mathcal{N}$ содержащей $L$ слоев назовем $l$ слой $\mathcal{L}_l$. Так $\mathcal{L}_1$ - входной слой, а $\mathcal{L}_L$ - выходной слой. Слои со $2$ по $L-1$ - это скрытые слои. На вход сети $\mathcal{N}$ подается $x \in \mathcal{X}$, где $X \subseteq \mathbb{R}^d$, то есть из пространства размерности $d$. 

Слои с $2$ по $L-1$ свёрточной сети $\mathcal{N}$ представляют собой свёртки, пулинги и нормализации. Каждый из которых по своей сути можно представить в виде линейных операций. Также между слоями применяются кусочно линейные функции активации.

На выходе сети $\mathcal{N}$ получается вектор $a \in \mathcal{Y}$, где $Y \subseteq \mathbb{R}^{n_L}$, то есть пространство размерности $n_L$. Выходной слой сети $\mathcal{L}_l$ применяет функцию softmax для получения выходного вектора. 

CNN работает как классификатор $F: \mathcal{X} \rightarrow \mathcal{Y}$, сопоставляющий элементам из множества $\mathcal{X}$ элементы из множества $\mathcal{Y}$. При этом $F$ представляет собой сложную систему с трудом понимаемую человеком.

Таким образом перед работой стоит задача построить интерпретацию CNN $\mathcal{N}$ в виде линейной модели $\mathcal{M}$, хорошо поддающейся интерпретации. Где под интерпретацией понимается выделение важных признаков и классификация близких объектов одним и тем же классификатором. Таким образом для $\mathcal{M}$ выдвигаются два требования:
\begin{enumerate}
    \item \textbf{Точность}:
    Модель $\mathcal{M}$ математически эквивалента модели $\mathcal{N}$.
    \item \textbf{Консистентность}:
    Модель $\mathcal{M}$ даёт близкие интерпретации для близких объектов выборки.
\end{enumerate}

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}